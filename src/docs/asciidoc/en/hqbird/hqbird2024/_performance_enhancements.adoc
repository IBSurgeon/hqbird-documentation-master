[[_hqbird_performance]]
= Performance enhancements

== Pool of external connections

HQbird supports a pool of external connections for Firebird 2.5 and Firebird 3. The standard build of Firebird 4.0 and higher supports the creation of external connection pools out of the box.

An external connection pool allows execute `EXECUTE ON EXTERNAL` statements with less overhead in reconnecting to the external database.

[NOTE]
====
Please note -- this pool is allocated per Firebird instance.
====

The feature is managed in the `firebird.conf`:

[source]
----
# ============================
# Settings of External Connections Pool
# ============================

# Set the maximum number of inactive (idle) external connections to retain at
# the pool. Valid values are between 0 and 1000.
# If set to zero, pool is disabled,
# i.e. external connection is destroyed immediately after the use.
#
# Type: integer
#
#ExtConnPoolSize = 0

# Set the time before destroying inactive external connection, seconds.
# Valid values are between 1 and 86400.
#
# Type: integer
#
#ExtConnPoolLifeTime = 7200
----

From the application point of view, no additional steps are required to use or do not use -- it is enabled or disabled in the server configuration, and absolutely seamless for the applications.

The following commands exist to manage pool:

* changes the pool size
+
[source]
----
ALTER EXTERNAL CONNECTIONS POOL SET SIZE N
----
+
Example -- this command sets the size of a pool to 190 connections.
+
[source,sql]
----
ALTER EXTERNAL CONNECTIONS POOL SET SIZE 190
----
* changes the lifetime of the pooled connection
+
[source]
----
ALTER EXTERNAL CONNECTIONS POOL
SET LIFETIME N {SECOND | MINUTE | HOUR}
----
+
Example -- this command limits the lifetime of a connection in the pool to 1 hour.
+
[source,sql]
----
ALTER EXTERNAL CONNECTIONS POOL SET LIFETIME 1 HOUR
----
* clear all pooled connections
+
[source,sql]
----
ALTER EXTERNAL CONNECTIONS POOL CLEAR ALL
----
* clear the oldest connection in the pool
+
[source,sql]
----
ALTER EXTERNAL CONNECTIONS POOL CLEAR OLDEST
----

To get information about pool status, new context variables were introduced.
The following example demonstrates their usage

[source,sql]
----
SELECT
  CAST(RDB$GET_CONTEXT('SYSTEM', 'EXT_CONN_POOL_SIZE') AS INT) AS POOL_SIZE,
  CAST(RDB$GET_CONTEXT('SYSTEM', 'EXT_CONN_POOL_IDLE_COUNT') AS INT) AS POOL_IDLE,
  CAST(RDB$GET_CONTEXT('SYSTEM', 'EXT_CONN_POOL_ACTIVE_COUNT') AS INT) AS POOL_ACTIVE,
  CAST(RDB$GET_CONTEXT('SYSTEM', 'EXT_CONN_POOL_LIFETIME') AS INT) AS POOL_LIFETIME
FROM RDB$DATABASE;
----

<<<

== Cached prepared statements

HQbird has the feature to improve the performance of Firebird (versions 4.0 and 5.0) engine in case of the many
frequent and fast SQL queries: server-side cache of prepared SQL statements. Starting with Firebird 5.0, this feature is available in the standard out-of-the-box build.

=== Cached prepared statements in Firebird 3.0 and 4.0

This feature can be enabled in `firebird.conf` with the parameter `DSQLCacheSize`:

[source]
----
# Size of DSQL statements cache.
# Maximum number of statements to cache.
# Use with care as it is per-attachment and could lead to big memory usage.
# Value of zero disables caching.
# Per-database configurable.
# Type: integer
#DSQLCacheSize = 0
----

The number specifies how many recent queries for each database connection to cache.

To apply the new value, Firebird restart is required.

By default, cache of prepared statements is 0, it means OFF.
We recommend to carefully enable it: start with values like 4, 8, 16, to find the best performance effect.

[NOTE]
====
Please note: enabling cache of prepared statements increases the memory usage.
====

=== Cached prepared statements in Firebird 5.0

In Firebird 5.0, the prepared statement cache was redesigned and included in the `"vanilla"` build.

The cache of prepared statements is controlled by the `MaxStatementCacheSize` parameter in `firebird.conf`.

----
# ----------------------------
# Maximum statement cache size
#
# The maximum amount of RAM used to cache unused DSQL compiled statements.
# If set to 0 (zero), statement cache is disabled.
#
# Per-database configurable.
#
# Type: integer
#
#MaxStatementCacheSize = 2M
----

By default, the prepared statement cache is enabled. To disable it, you need to set the `MaxStatementCacheSize` parameter to 0.

<<<

== TempSpaceLogThreshold: monitoring of big sorting queries and BLOBs

HQbird has a new parameter in `firebird.conf` in Firebird 2.5, Firebird 3.0, Firebird 4.0 and Firebird 5.0:

[source]
----
TempSpaceLogThreshold=1000000000 #bytes
----

When Firebird sees this parameter, it starts to log to `firebird.log` queries which produce large sortings: queries with `GROUP BY`, `ORDER BY`, etc.

When such query creates the sorting file which exceeds the specified threshold, the following message will appear in `firebird.log`:

----
SRV-DB1	Wed Nov 28 21:55:36 2018
	Temporary space of type "sort" has exceeded threshold of 1000000000 bytes.
	Total size: 10716980736, cached: 1455423488 bytes, on disk: 9263120384 bytes.
	Query: select count(*) from (select lpad('',1000,uuid_to_char(gen_uuid())) s
	       from rdb$types a,rdb$types b, rdb$types c  order by 1)
----

*Total size*:: the total size of sorting file

*Cached*:: the part of sorting which had fit into temporary space (specified by `TempCacheLimit` parameter)

*On disk*:: the part of sorting which was stored to the temporary file, which can be cached in the OS memory, or stored on disk (in the folder specified by `TempDirectories` parameter, or in the default temp folder)

For very big BLOBs the following message will appear in the `firebird.log`

----
SRV-DB1	Tue Nov 27 17:35:39 2018
	Temporary space of type "blob" has exceeded threshold of 500000000 bytes.
	Total size: 500377437, cached: 0 bytes, on disk: 501219328 bytes.
----

Use `TempSpaceLogThreshold` to find the non-optimized queries with big sortings and big BLOBs.
Starting with Firebird 3.0 it will also report large hash tables (those caused by HASH JOINs).

If you encounter such queries, optimize them either with redesign of SQL query itself, or try to enable parameter `SortDataStorageThreshold`.

<<<

== SortDataStorageThreshold: REFETCH instead SORT for wide record sets

HQbird supports the new REFETCH optimization method. The standard build of Firebird version 4.0 and higher supports this optimization algorithm out of the box.

HQbird has a new parameter `SortDataStorageThreshold` in `firebird.conf` (Firebird 3.0+):

[source]
----
SortDataStorageThreshold=16384 # bytes
----

[NOTE]
====
Since version 4.0 this parameter has been renamed to `InlineSortThreshold`.

[source]
----
InlineSortThreshold=16384 # bytes
----
====

If the size of the record, returned by SQL query, will be more than specified threshold, Firebird will use the different approach for sorting record sets: REFETCH instead of SORT.

For example, we have the following query

[source]
----
select tdetl.name_detl
    ,tmain.name_main
    ,tdetl.long_description
from tdetl
join tmain on tdetl.pid=tmain.id
order by tdetl.name_detl
----

with the following execution plan:

----
Select Expression
    -> Sort (record length: 32860, key length: 36)
        -> Nested Loop Join (inner)
            -> Table "TMAIN" Full Scan
            -> Filter
                -> Table "TDETL" Access By ID
                    -> Bitmap
                        -> Index "FK_TABLE1_1" Range Scan (full match)
----

In this case, the size of each record to be sorted is 32860+36 bytes.
It can lead to the very big sort files, which will be written to the disk, and the overall query can slow.

With parameter `SortDataStorageThreshold=16384` or `InlineSortThreshold=16384`, Firebird will use plan REFETCH, where only key is sorted, and data are read from the database:

----
Select Expression
    -> Refetch
        -> Sort (record length: 76, key length: 36)
            -> Nested Loop Join (inner)
----

This approach can significantly (2-5 times) speed up queries with very wide sorted record sets (usually, heavy reports).

.Please note!
[NOTE]
====
It is not recommended to set `SortDataStorageThreshold` (`InlineSortThreshold`) less than 2048 bytes.
====

<<<

[[_hqbird_performance_multi_threaded]]
== Multi-thread sweep, backup, restore

In HQbird, the possibility of multi-threaded execution of sweep, backup and restore has appeared, which speeds up their work from 2x to 6 times (depending on the specific database). Multi-threaded operations work in HQbird for Firebird 2.5, 3.0 and 4.0, in any architectures -- Classic, SuperClassic, SuperServer. For Firebird 5.0, multi-threaded operations are available in the "vanilla" version out of the box.

To enable multi-threaded execution, the `gfix` and `gbak` command-line utilities have the `–par _n_` option, where `n` is the number of threads that will be involved in a particular operation. In practice, choosing the number n should be correlated with the number of available processor cores.

For example

* `gfix –sweep database –par 8 ...`
* `gbak –b database backup –par 8 ...`
* `gbak –c backup database –par 8 ...`

Also, to control the number of threads and set their default number in `firebird.conf`, two new parameters
are introduced that affect only sweep and restore, but not backup:

----
# ============================
# Settings for parallel work
# ============================
#  Limit number of parallel workers for the single task. Per-process.
#  Valid values are from 1 (no parallelism) to 64. All other values
#  silently ignored and default value of 1 is used.
MaxParallelWorkers = 64
----

Example: if you set `MaxParallelWorkers = 10`, then you can

* run `gfix –sweep database –par 10`
* run `gfix –sweep database –par 5` and `gbak –c –par 5 ...`

That is, no more than 10 threads will be used in total.
In case of exceeding (for example, if you set 6 threads for sweep and 6 threads for restore), for a process that exceeds the limit, the message "`No enough free worker attachments`" will be displayed).

Thus, to enable the multi-threaded capabilities of sweep and restore, you must set the `MaxParallelWorkers` parameter in `firebird.conf`

----
MaxParallelWorkers = 64
----

and then restart Firebird.

The `ParallelWorkers` sets the number of threads used by sweep and restore by default if the `–par _n_` option is not specified.

----
#  Default number of parallel workers for the single task. Per-process.
#  Valid values are from 1 (no parallelism) to MaxParallelWorkers (above).
#  Values less than 1 is silently ignored and default value of 1 is used.
#
ParallelWorkers = 1
----

For example, if `ParallelWorkers = 8`, then starting

----
gfix –sweep
----

without the `–par _n_` option will use 8 threads to execute sweep in parallel.

[IMPORTANT]
====
For restore, filling tables from backup is always performed in one thread, and only creating indexes is parallelized.
Thus, the acceleration for restore depends on the number of indexes in the database and their size.
Also, the `ParallelWorkers` parameter automatically affects the creation of indexes performed by the `CREATE INDEX` and `ALTER INDEX ... ACTIVE` operations.
====

As mentioned above, these options do not affect backup.
The multi-threading of backup is regulated only by the `–par _n_` parameter in the command line:

* `gbak –b –par 6 ...`
* `gbak –b –par 8 –se ...`


[IMPORTANT]
====
If the database is in shutdown single state, when only 1 connection is allowed to the database, then in version 2.5 both sweep and backup with `–par _2_` or more will produce an error several seconds after starting:

* sweep -- connection lost to database
* backup -- ERROR: database ... shutdown (via xnet protocol, a line with this message will not be displayed in the backup log)

This is due to the fact that for these operations an appropriate number of database connections is required, more than 1.

In 3.0, only backup will throw an error "`ERROR: database ... shutdown`", sweep will work.

Multi-threaded restore, Firebird 2.5, 3.0 and 4.0, creates the database in shutdown multi mode, so such errors do not occur.
However, there is a risk of connecting other applications from SYSDBA or the owner to the database in the restore process.
====

.Notes
[NOTE]
====
* The new parameters in `firebird.conf` only affect sweep and restore, to simplify administration and eliminate
ambiguity, it is recommended that you always explicitly specify the `–par n` parameter
for `gfix` and `gbak` if you need to perform multi-threaded sweep, restore, and backup operations.
For example, if you set `ParallelWorkers = 4` and do not specify `–par n`, then sweep and restore will use 4
threads by default, and backup will use 1 thread, because it does not use the values from `firebird.conf` neither
locally nor with `–se`.
* The performance improvement does not necessarily depend on the number of processor cores and their compliance
with the set value `–par n`. It depends on the number of cores, the Firebird architecture, and the disk subsystem
performance (IOPS). Therefore, the optimal value `–par n` for your system must be selected experimentally.
====

<<<

[[_hqbird_blob_append]]
== BLOB_APPEND function

Regular operator `||` (concatenation) with BLOB arguments creates temporary BLOB per every pair of args
with BLOB. This could lead to the excessive memory consumption and growth of database file. The `BLOB_APPEND` function is designed to concatenate BLOBs without creating intermediate BLOBs.

In order to achieve this, the result BLOB is left open for writing instead of been closed immediately after it is filled with data. I.e. such blob could be appended as many times as required. Engine marks such blob with new internal flag `BLB_close_on_read` and closes it automatically when necessary.

*Available in*: DSQL, PSQL.

.Syntax:
----
BLOB_APPEND(<blob> [, <value1>, ... <valueN]>
----

.Parameters of BLOB_APPEND function
[cols="1,2", options="header"]
|===
| Parameter
| Description

| blob
| BLOB or NULL.

| value
| Any type of value.
|===


*Return type*: BLOB, temporary, not closed (i.e. open for writting), marked by flag
`BLB_close_on_read`.

Input Arguments:

* The first argument is BLOB or NULL. The following options are possible:
** NULL:  creates new temporary blob, not closed, with flag `BLB_close_on_read`
** permanent BLOB (from table) or temporary already closed BLOB:
will create a new empty unclosed BLOB with the flag `BLB_close_on_read` and the contents of the first BLOB will be added to it
** temporary unclosed BLOB with the `BLB_close_on_read` flag: it will be used further
* other arguments can be of any type. The following behavior is defined for them:
** NULL ignored
** non-BLOBs are converted to string (as usual) and appended to the content of the result
** BLOBs, if necessary, are transliterated to the character set of the first argument and their contents are appended to the result

The `BLOB_APPEND` function returns a temporary unclosed BLOB with the` BLB_close_on_read` flag.
This is either a new BLOB or the same as in the first argument. Thus, a series of operations like `blob = BLOB_APPEND (blob, ...)` will result in the creation of at most one BLOB
(unless you try to add a BLOB to itself).
This BLOB will be automatically closed by the engine when the client tries to read it, assign it to a table, or use it in other expressions that require reading the content.

[NOTE]
====
Testing a BLOB for NULL value using the `IS [NOT] NULL` operator does not read it, and therefore a temporary BLOB with the` BLB_close_on_read` flag will not be closed during such test.
====

[source,sql]
----
execute block
returns (b blob sub_type text)
as
begin
  -- will create a new temporary not closed BLOB
  -- and will write to it the string from the 2nd argument
  b = blob_append(null, 'Hello ');
  -- adds two strings to the temporary BLOB without closing it
  b = blob_append(b, 'World', '!');
  -- comparing a BLOB with a string will close it, because for this you need to read the BLOB
  if (b = 'Hello World!') then
  begin
  -- ...
  end
  -- will create a temporary closed BLOB by adding a string to it
  b = b || 'Close';
  suspend;
end
----


[TIP]
====
Use the `LIST` and` BLOB_APPEND` functions to concatenate BLOBs. This will save memory consumption, disk I/O,
and prevent database growth due to the creation of many temporary BLOBs when using concatenation operators.
====


[example]
====
Let's say you need to build JSON on the server side. We have a PSQL package `JSON_UTILS` with a set of functions for converting primitive data types to JSON notation.
Then the JSON building using the `BLOB_APPEND` function will look like this:

[source,sql]
----
EXECUTE BLOCK
RETURNS (
    JSON_STR BLOB SUB_TYPE TEXT CHARACTER SET UTF8)
AS
  DECLARE JSON_M BLOB SUB_TYPE TEXT CHARACTER SET UTF8;
BEGIN
  FOR
      SELECT
          HORSE.CODE_HORSE,
          HORSE.NAME,
          HORSE.BIRTHDAY
      FROM HORSE
      WHERE HORSE.CODE_DEPARTURE = 15
      FETCH FIRST 1000 ROW ONLY
      AS CURSOR C
  DO
  BEGIN
    SELECT
      LIST(
          '{' ||
          JSON_UTILS.NUMERIC_PAIR('age', MEASURE.AGE) ||
          ',' ||
          JSON_UTILS.NUMERIC_PAIR('height', MEASURE.HEIGHT_HORSE) ||
          ',' ||
          JSON_UTILS.NUMERIC_PAIR('length', MEASURE.LENGTH_HORSE) ||
          ',' ||
          JSON_UTILS.NUMERIC_PAIR('chestaround', MEASURE.CHESTAROUND) ||
          ',' ||
          JSON_UTILS.NUMERIC_PAIR('wristaround', MEASURE.WRISTAROUND) ||
          ',' ||
          JSON_UTILS.NUMERIC_PAIR('weight', MEASURE.WEIGHT_HORSE) ||
          '}'
      ) AS JSON_M
    FROM MEASURE
    WHERE MEASURE.CODE_HORSE = :C.CODE_HORSE
    INTO JSON_M;

    JSON_STR = BLOB_APPEND(
      JSON_STR,
      IIF(JSON_STR IS NULL, '[', ',' || ascii_char(13)),
      '{',
      JSON_UTILS.INTEGER_PAIR('code_horse', C.CODE_HORSE),
      ',',
      JSON_UTILS.STRING_PAIR('name', C.NAME),
      ',',
      JSON_UTILS.TIMESTAMP_PAIR('birthday', C.BIRTHDAY),
      ',',
      JSON_UTILS.STRING_VALUE('measures') || ':[', JSON_M, ']',
      '}'
    );
  END
  JSON_STR = BLOB_APPEND(JSON_STR, ']');
  SUSPEND;
END
----

A similar example using the usual concatenation operator `||` is an order of magnitude slower and does 1000 times more disk writes.
====

<<<

[[_hqbird_performance_left_to_inner]]
== Transform LEFT joins into INNER

HQbird allow transform `LEFT` joins into `INNER` ones if the `WHERE` condition violates the outer join rules.

Example:

[source,sql]
----
SELECT *
FROM T1 LEFT JOIN T2 ON T1.ID = T2.ID
WHERE T2.FIELD1 = 0
----

In this case the condition `T2.FIELD1 = 0` effectively removes all the "fake NULL" rows of T2, so the result is the same
as for the `INNER JOIN`. However, the optimizer is forced to use the T1->T2 join order while T2->T1 could also be
considered. It makes sense to detect this case during join processing and internally replace `LEFT` with `INNER` before optimization starts.

This is primarily intended to improve "ad hoc" and machine-generated (e.g. ORM) queries.

[NOTE]
====
This optimization will not be enabled if a NULL value is checked, for example

[source,sql]
----
SELECT *
FROM T1 LEFT JOIN T2 ON T1.ID = T2.ID
WHERE T2.ID IS NULL
----

or

----
SELECT *
FROM T1 LEFT JOIN T2 ON T1.ID = T2.ID
WHERE T2.ID IS NOT NULL
----
====

