[[_hqbird_struct_analyze]]
= Анализ структуры базы данных

== Обзор структуры базы данных Firebird

Первое, что следует сказать о структуре базы данных Firebird, это то, что она представляет собой набор страниц строго определенного размера: 4096, 8192, 16384 или 32768 (начиная с Firebird 4.0).

Страницы могут быть разных типов, каждая из которых служит определенной цели.

Страницы одного типа не идут строго одна за другой -- они могут быть перемешаны, и размещены в файле в том порядке, в котором они были созданы сервером при расширении или создании базы данных.

image::5.1.1.png[]

.Типы страниц
[cols="1,1,1", options="header"]
|===
| Тип страницы
| ID
| Описание

|**pag_undefined**
|0
|Undefined - Если страница имеет этот тип страницы, скорее всего, она пуста.

|**pag_header**
|1
|Database header page

|**pag_pages**
|2
|Page inventory page (or Space inventory page – SIP)

|**pag_transactions**
|3
|Transaction inventory page (TIP)

|**pag_pointer**
|4
|Pointer page

|**pag_data**
|5
|Data page

|**pag_root**
|6
|Index root page

|**pag_index**
|7
|Index (B-tree) page

|**pag_blob**
|8
|Blob data page

|**pag_ids**
|9
|Gen-ids

|**pag_log**
|10
|Write ahead log information
|===

<<<

== Как анализировать структуру базы данных с помощью HQbird Database Analyst (IBAnalyst)

*IBAnalyst* — это инструмент, который помогает пользователю детально анализировать статистику базы данных Firebird и выявлять возможные проблемы с производительностью базы данных, ее обслуживанием и взаимодействием приложения с базой данных.

IBAnalyst графически отображает статистику базы данных Firebird в удобной для пользователя форме и выявляет следующие проблемы:

* фрагментирование таблиц и BLOB,
* версии записей,
* сборка мусора,
* эффективность индексов

Более того, IBAnalyst может автоматически вносить разумные предложения по улучшению производительности и обслуживанию базы данных.

IBAnalyst может получать статистику из действующих производственных баз данных через Services API (рекомендуется) или анализировать текстовый вывод команд `gstat -a -r ...`.
Статистика в периоды пиковой нагрузки может дать много информации о реальных проблемах с производительностью в производственных базах данных.

Основные возможности IBAnalyst перечислены ниже:

* Получение статистики базы данных через Service API и из вывода `gstat`.
* Сводка всех актуальных и возможных проблем в базе данных
* Цветная сетка для представления таблиц, индексов и таблиц->индексов, которая выделяет фрагментированные таблицы, плохие индексы и т. д.
* Автоматическая экспертиза статистики базы данных предоставляет рекомендации и инструкции по следующим вопросам:
** Оптимальный размер страницы базы данных
** Состояние транзакций и критический разрыв между транзакциями.
** Различные флаги базы данных
** Глубина индекса
** Дубликаты ключей индекса
** Фрагментированные таблицы
** Версии записи
** Очень большие таблицы
* и другое

=== Как правильно получать статистику из базы данных Firebird

==== Правильное время и место

Звучит странно, но просто взять статистику через gstat или Services API недостаточно. Статистику необходимо собирать в нужный момент, чтобы показать, как приложения влияют на данные и транзакции в базе данных.

Худшее время для сбора статистики

* Сразу после восстановления
* После снятия резервной копии (`gbak –b db.gdb`) без указания переключателя `–g`
* После ручного sweep (`gfix –sweep`)

Также верно, что во время работы могут быть моменты, когда база данных находится в правильном состоянии, например, когда приложения нагружают базу данных меньше, чем обычно (пользователи обедают, ужинают или в определенное время бизнес-процесса).

Как обнаружить, что в базе данных что-то не так?

Yes, your applications can be designed so perfect that they will always work with transactions and data correctly, not making sweep gaps, lot of active transactions, long running snapshots and so on.
Usually it does not happen.
At least because some developers test their applications running 2-3 simultaneous users at the same time, not more.
Thus, when they set up written applications for 15 and more simultaneous users, database can behave unpredictably.
Of course, multi-user mode can work Ok, because most of multi-user conflicts can be tested with 2-3 concurrently running applications.
But, next, when more concurrent applications will run, garbage collection problems can come (at least). And this can be caught if you take statistics at the correct moments.

==== If you does not experience periodical performance problems

This can happen when your applications are designed correctly, there is low database load, or your hardware is modern and very powerful (enough to handle well current user count and data).

The most valuable information is transactions load and version accumulation.
This can be seen only if you setup regular statistics saving.

The best setup is to get hourly transaction statistics.
This can be done by running

[source,bash]
----
gstat –h db.gdb > db_stat_<time>.txt
----

where

* `db.gdb` is your database name,
* `db_stat_<time>.txt` is text file where statistics will be saved,
* `<time>` -- current date and time when statistics was taken.

Also you can schedule to gather database statistics with HQbird FBDataGuard, job "`Database: Statistics`".

==== If you experience periodical performance problems

These problems usually caused by automatic sweep run.
First you need to determine time period between such a performance hits.
Next, divide this interval minimally to 4 (8, 16 and so on). Now information systems have lot of concurrent users, and most of performance problems with not configured server and database happens 2 or 3 timers per day.

For example, if performance problem happens each 3 hours, you need to take

[source,bash]
----
gstat –h db.gdb
----

statistics each 30-45 minutes, and

[source,bash]
----
gstat –a –r db.gdb –user SYSDBA –pass masterkey
----

each 1-1.5 hour.
The best is when you take `gstat –a –r` statistics right before forthcoming performance hit.
It will show where real garbage is and how many obsolete record versions accumulated.

==== What to do with this statistics

If your application explicitly uses transactions and uses them well, i.e.
you know what is `read read_committed` and when to use it, your snapshot transactions lasts no longer than needed, and transactions are being active minimal duration of time, you can tune sweep interval or set it off, and then only care about how many updates application(s) makes and what tables need to be less updated or cared about updates.

What does this mean, you can ask? We'll give example of some system, where performance problems happened each morning for 20-30 minutes.
That was very sufficient for `morning` applications, and could not last longer.

Database admin was asked correct questions, and here is the picture:

Daily work was divided by sections -- analytic works in the morning, than data is inserted and edited by usual operators, and at the end of the day special procedures started gathering data, that would be used for analytic next day (at least).

The last work on database at the end of day was lot of updates, and updates of those tables which analytic used in the morning.
So, there were a lot of garbage versions, which started to be collected by application, running in the morning.

And, the answer to that problem was found simple -- to run `gfix –sweep` at the end of the day.

Sweep reads all tables in database and tries to collect all garbage versions for committed and rolled back transactions.
After sweeping database became clear nearly it comes after restore.

And, "`morning problem`" has gone.

So, you need to understand statistics with lot of other factors:

* how many concurrent users (average) work during the day
* how long is the working day (8, 12, 16, 24 hours)
* what kind of applications running at different day times, and how they affect data being used by other applications, running at the same time or next. I.e. you must understand business processes happening during the whole day and whole week.


==== When DBA can't do nothing

Sadly to say, these situations happen.
And again, example:

Some system installed for ~15 users.
Periodically performance is so bad, that DBA needs to restart server.
After server restart everything works fine for some time, then performance gets bad again.
Statistics showed that average daily transactions is about `75,000`, and there are active transactions running from the start of day to the moment when performance getting down.

Unfortunately, applications were written with BDE and with no transactions using at all; i.e.
all transaction handling was automatic and used by BDE itself.
This caused some transactions to stay active for a long time, and garbage (record versions) accumulated until DBA restarted server.
After restart the automatic sweep will start, and the garbage will be collected (eliminated).

All these was caused by applications, because they were tested only with 2-3 concurrent users, and when they became ~15, applications started to make very high load.

Need to say that in that configuration 70% of users were only reading data, and other 30% were inserting and updating some (!) data.

In this situation the only thing that can make performance better is to redesign transaction management in this application.

==== How IBAnalyst can help find problems in your Firebird database

Let's walk through the key features of IBAnalyst.
When you look at your database statistics in IBAnalyst first time, things can be not clear, especially if IBAnalyst shows lot of warnings by colored red and yellow cells at Summary, Tables and Index views.
Let's consider several real statistics examples.

=== Summary View

Summary contains the most important information extracted from database statistics.
Usually full statistics of database contains hundreds of Kbytes and it is not easy to recognize the important information.

Below is the description of database objects and parameters that you may see in Summary.
For description of visible problems (marked *red* or **yellow**) see column hints or Recommendations output.



[cols="1,1", options="header"]
|===
| Object or parameter
| Description


|**Database name**
|Name of analyzed database.

|**Creation date**
|Database creation date. When it was created by `CREATE DATABASE` statement or restore (`gbak -c` or `gbak -r`).

|**Statistics date**
|When statistics was taken -- statistics file date or Services API call date (now).

|**Page size**
|Page size is the physical parameter of database. The best page size is 4096 or 8192 bytes. Other page sizes (less than 4096) marked as red. For better performance restore database from backup using 4K or 8K page size. (Note: Firebird __2.0+ can use 16K page size__).

|**Forced Write**
|It shows the mode of changed pages writing: synchronized or asynchronized -- appropriate setting is ON or OFF. OFF is not recommended, because server crush, power failure or other problems can cause database corruption.

|**Dialect**
|Current database dialect.

|**Sweep interval**
|Current sweep interval value. Marked yellow if it is not 0, and marked red if Sweep Gap greater than Sweep interval.

|**On Disk Structure**
|ODS. It is a database physical format. See hint to know ODS number for particular IB/FB versions

|**Transaction block**
|

|**Oldest transaction**
|

Oldest interesting transaction.

The oldest transaction id that was rolled back, or in limbo.

|**Oldest snapshot**
|

Oldest snapshot transaction

Id of transaction that was oldest active when currently oldest snapshot started.

|**Oldest active**
|

Oldest active transaction

Id of oldest still active transaction.

|**Next transaction**
|Newest available transaction id

|**Sweep gap (snapshot – oldest)**
|For ODS 10.x databases. Difference between Oldest Snapshot and Oldest Interesting transaction. If it is greater than sweep interval, and sweep interval is > 0, Firebird tries to run sweep, and it can slowdown performance.

|**Snapshot gap (active – oldest)**
|Difference between Oldest Active and Oldest transaction. Same as previous sweep gap.

|**TIP size**
|Transaction Inventory Page size, in pages and kilobytes. TIP holds transaction state for every transaction was started from database creation (or restore). It is computed as Next transaction div 4 (bytes).

|**Snapshot TIP Size**
|Size of Transaction Inventory Pages that needed for snapshot transactions. Indicates how much memory will take each snapshot transaction to check concurrent transactions state.

|**Active transactions**
|Currently active (on the moment when statistics was taken from database) transaction count (Next – Oldest Active). Maybe incorrect, because it can be one active transaction and lot of ahead transactions committed. Anyway, active transactions prevent garbage collection.

|**Transactions per day**
|Simply divides Next transaction by days' count between database creation date and date statistics taken. Shows average transaction per day, and useless if it is not production database. Transaction warnings mostly based on average transactions per day count.

|**Data versions percent**
|Percent of record versions in database. Also total records size and versions size for all tables is shown, and total index size. Row is not shown when statistics does not contain record count information (`gstat -a` without `-r` option). Note that there can be lot of other data (transaction inventory pages, empty pages and so on) in your database.

|**Table/Index lists **(also reported in recommendations)
|

|**Fragmented Tables**
|Here you can view tables (with data > 200 kilobytes) that have average fill less than 60% (File/Options/Table average fill).

|**Versioned Tables**
|List of tables that have Versions greater than Records, set in Options/Tables.

|**Tables fragmented with blobs**
|List of tables that have blob fields with data size less than database page size.

|**Massive deletes/updates**
|List of tables that had lot of data deleted/updated by one delete/update statement.

|**Very big tables**
|Tables that are close to technical InterBase limit (36 gigabytes per table). You will see warning beforehand problem can occur.

|**Deep Indices**
|Indices with depth more than 3 (Options/Index)

|**Bad Indices**
|Indices with big MaxDup and TotalDup values

|**Broken or incomplete indices**
|Indices with key count less than record count. This can happen when index is broken or when statistics is taken during index creation or re-activation.

|**Useless Indices**
|Indices with Unique column = 1. May be deleted or deactivated, because they are useless for index search or sort operations.

|**Tables with no records**
|List of tables with Records = 0. This can be by design (temporary tables), or they can be just forgotten by database developer.
|===

image::5.2.1.png[]


Summary page shows a lot of information, but the most valuable is transactions state (__please read description of possible transactions states in IBAnalyst help, it is available by clicking F1 or in menu Help__).

At this screenshot you can see that some transaction is active for a long time, "`60% of daily average`".
IBAnalyst marks such transaction's state by red, because this transaction may prevent accumulated versions to be considered as garbage by server, and so, to be garbage collected.
This is a possible reason of slowness: the more versions exist for some record, the more time it will takes to read it.

In order to find this long-running transaction you can use MON$Logger module of FBScanner, or perform direct query of `MON$` tables.
Then, to find out which tables were affected by long running transactions (tables with a lot of record versions) you need to go to "`Tables`" view of IBAnalyst.

=== Tables view

View *Tables* contains the information about all database tables.
It represents important statistical information about each table.
All table warnings are marked (see details below).

You can see the following columns (Columns *Records, RecLength, VerLen, Versions, Max Vers* are visible only if statistics was generated with `gstat -r` or with "`Include record/rec versions`" checkbox enabled):

[cols="1,1", options="header"]
|===
| Column
| Description


|**Records**
|Record count. Marked pink if table fragmented by blob fields which data is less than database page size. Hint shows real table fragmentation and average records if there were no blob fields. Such fragmentation can cause bad performance for big table joins or natural scans.

|**RecLength**
|Average record length. Depends on record data, especially on char/varchar columns data. Min physical record length is 17 bytes (record header + all fields are null), max – as declared in table. Statistics show this data without record header count, in this case RecLength can be 0 (if nearly all records are deleted)

|**VerLen**
|Average record version length. If it is close to RecLength, almost all record is being updated. If VerLen is 40-80% and not greater of RecLength, then Versions are mostly updates. If VerLen greater than 80-90% of RecLength, than maybe Versions are mostly deletes, or update is made by char/varchar columns with new, greater data. Marked *yellow* if it's size is greater than specified % (Options/Record/Version size) of average record size.

|**Versions**
|Current record version count. More versions slowdown table reads. Also lot of versions means that there is no garbage collection performed or records are not read by anyone. Marked *red* if version count is greater than Records. (Options/Record Versions).

|**Max Vers**
|Max record versions for one particular record. Marked blue when it is equal to 1 and Versions is non-zero. It means that there were massive update/delete operation. See Options, Table, Massive deletes updates option.

|**Data Pages**
|Allocated data pages

|**Size, Mb**
|`DataPages * Page Size`, in megabytes. I.e. this is total table size, records + versions. Graph shows percentage of that table from the whole data size.

|**Idx Size, Mb**
|Sum of all indices size for that table. Graph shows percentage of that value to total size of all indices.

|**Slots**
|Count of links to data pages. Empty links are Slots-Data Pages. Doesn't affect disk space or performance.

|**Average Fill**
|Average data page fill %. Can be computed as `(DataPages * Page_Size)/ Records * RecLength`. Low page fill means that table is "fragmented". Frequent updates/deletes can fragment data pages. Marked red if average fill rate is less than 60% (go to Options/Average Fill to adjust it). Marked yellow if it is an artifact of high table fragmentation when it's record is too small (11-13 bytes).

|**Real Fill**
|Because we found that Average Fill, calculated by `gstat`, sometimes gives wrong results (at least for tables with small blobs), we placed here calculated column, that counts average fill not by data pages, but by records+versions, including record header.

|**20%, 40%, 60% and 100% fill**
|Shows page count having corresponding fill rate. Can be turned on/off in Options dialog

|**Total %**
|How big is that table plus it's indices in %, related to other tables.
|===

image::5.2.2.png[]


At "`Tables`" view you can see tables and their important parameters: number of records, number of record versions, record length, maximum number of versions, etc.

You can sort this view to find the largest tables.
Especially we are interested tables with many record versions – many record versions will make garbage collection for affected tables longer.
Usually it is necessary to change update and delete algorithms to get rid of many record versions.

Row Versions show total versions count for particular table, and row Max Vers shows maximum versions reached by some record.
For example, if you look at table `SITE`, there are 40611 records, total versions are 76142, but one record has 501 versions.
Reading and parsing such packet from disk takes more time, so, reading this record is slower than reading others.

This picture also shows a lot of tables where data was deleted.
But, because of long running transaction, server can't delete these versions, and they still on disk, still indexed, and still being read by server when reading data.

=== Index view

View *Indices* represents all indices in your database.
You can estimate the effectiveness of indices with the following parameters (problem indices are marked *red* -- see smart hints for details)

[cols="1,1", options="header"]
|===
| Column
| Description


|**Depth**
|Index depth is the page count that engine reads from disk to walk from index root to record pointer. Optimal index depth is 3 or less. When Depth is 4 and higher, it is recommended to increase database page size (backup, then restore with `-page_size` option). This column will be marked red if index depth is greater than 3 (Options/Index/Index Depth). More chances to exceed optimal depth have indices built on long char/varchar columns.

|**Keys**
|Index key count. Usually equals to Records. If Keys is bigger
                                    than Records and Versions count is greater than 0 it means that
                                    concrete field value was changed in those record versions. If
                                    Table.RecVersions is bigger than Keys, than this index field(s)
                                    are not changed during updates.

|**KeyLen**
|Average index key length. The less KeyLen, the more equal or similar (postfix) values (keys) stored in index.

|**Max Dup**
|Maximum duplicates count for particular key value. Some old `gstat` versions show no more than 32767 or 65535 -- this bug is fixed in latest Firebird versions. Marked *red* if duplicates count is 30% of all keys. (Options/Index/Lot of key duplicates).

|**Total Dup**
|The overall count of keys with the same values.
Some old `gstat` versions show no more than 32767 or 65535 -- this bug is fixed in latest Firebird versions.

The closer this value to Keys count, the less effective will be searching using this index, especially when search is made using more than one index.
Total Dup value can be counted as Keys minus unique keys count (index statistics is nonlinear).

Marked *yellow* if `1/(Keys – TotalDup)` greater than 0.01, and red if in addition MaxDup is marked red too.
This constant (0.01) is used by optimizer (see sources in `opt.cpp`) as usable index selectivity border.
Optimizer will still use that index if none other index with better selectivity exists for some condition.

|**Uniques**
|Count of different key values. Primary and unique key indices will show same value as in Keys column. Useful to understand how many different values stored in index -- is it useful or not. Index is useless if Unique column shows 1 (marked yellow).

|**Selectivity**
|Information from `rdb$indises.rdb$statistics`, only visible if "`load table/index metadata`" was On. If selectivity stored in database differs from computed selectivity, *yellow* warning shown (less than 20% difference) or *red* (higher than 20% difference). *Blue* warning is shown when index is empty but it's selectivity is not 0. Selectivity of inactive indices are ignored.

|**Size, Mb**
|Index size in megabytes. Gap show percentage of that index size related to total size of all indices.

|**Average Fill**
|Average index pages fill rate, in %. Marked *red* if average fill rate is less than 50% (go to Options/Average Index Fill to adjust it). Fragmented index results more page reads as usual, and it's Depth can be higher. Can be fixed by alter index inactive/active, if it is not index created by primary, unique or foreign key constraints.

|**Leafs**
|Leaf page count (pages with keys and record pointers).

|**20%, 40%, 60% and 100% fill**
|Shows page count having corresponding fill rate. Can be turned on/off in Options dialog
|===

image::5.2.3.png[]


Some production databases can have indices with the only key value indexed.
This can happen because database was developed "`to be extended in the future`", or, someone just experimented with the indices during development or tests.
You can see these indices as "`Useless`" in IBAnalyst: `I_NUMBER`, etc, built on the column that has only one value for all rows.
These indices are really useless, because

* Optimizer may use this index if you specify "`where field =...`". Since field contains only one value, using index will cause useless reading of index pages from disk to memory, and consume memory (and time) when server will prepare which rows to show for that query.
* Creating indices is the part of restore process. Extra indices adds extra time.

Of course, that is not all that you can find about your database in IBAnalyst.
You can also find

* average number of transactions per day
* was there rollbacks or lost connections, and when
* how big (in megabytes) each table and index
* tables that have records interchanged by blobs, and thus reading only records is slower
* empty tables -- just forgotten, or empty at the time when statistics was taken
* indices with lot of duplicate keys (you can consider about column value distribution)
* indices with depth 4 and greater -- maybe you need to increase page size to speed up
